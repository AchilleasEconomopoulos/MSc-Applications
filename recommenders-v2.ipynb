{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863f3c51",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e05efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2bcf0",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafce2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "artists = pd.read_csv(\"data/artists.dat\", delimiter='\\t')\n",
    "tags = pd.read_csv(\"data/tags.dat\", delimiter='\\t')\n",
    "users_artists = pd.read_csv(\"data/user_artists.dat\", delimiter='\\t')\n",
    "users_taggedartists = pd.read_csv(\"data/user_taggedartists.dat\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13319070",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab7bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll consider an artistID as valid if:\n",
    "#   1. They've had minimal interaction with any userID\n",
    "#   2. We have metadata for that artistID\n",
    "valid_artists = set(users_artists['artistID']).intersection(set(artists['id']))\n",
    "\n",
    "# Drop any rows with invalid artistIDs from the user-artist-tag matrix\n",
    "#   i.e. We can't do CBF with an artist we don't have listening time for or that we can't visualize later on\n",
    "users_taggedartists = users_taggedartists[users_taggedartists['artistID'].isin(valid_artists)]\n",
    "users_artists = users_artists[users_artists['artistID'].isin(valid_artists)]\n",
    "\n",
    "\n",
    "# Making sure we have metadata for tags (for visualization later)\n",
    "valid_tags = set(users_taggedartists['tagID'])\n",
    "users_taggedartists = users_taggedartists[users_taggedartists['tagID'].isin(valid_tags)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244c81e",
   "metadata": {},
   "source": [
    "### Load artist and user IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e9483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting off, look into the user-artist-tag table to extract artist and tag IDs\n",
    "cb_artist_ids = set(users_taggedartists['artistID'])\n",
    "tag_ids = set(users_taggedartists['tagID'])\n",
    "\n",
    "\n",
    "# Do the same for CF. Also the final userset comes from this matrix.\n",
    "#   i.e. these are the users that we'll train and test the system on\n",
    "cf_artist_ids = set(users_artists['artistID'])\n",
    "user_ids = set(users_artists['userID'])\n",
    "\n",
    "# Then cross-check cf_artists with the artist table (need metadata to visualize)\n",
    "cf_artist_ids = cf_artist_ids.intersection(set(artists['id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19059b47",
   "metadata": {},
   "source": [
    "### Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d065cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 92834\n",
      "Train set size: 90957 -> 0.98\n",
      "Train set size: 1877 -> 0.02\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in users_artists.groupby('userID'):\n",
    "\n",
    "    # Minimum threshold to consider a user's data impactful\n",
    "    if len(group) < 5:\n",
    "        train_list.append(group)\n",
    "    else:\n",
    "        train, test = train_test_split(group, test_size=1, random_state=42)\n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "\n",
    "# Concatenate final datasets\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(f\"Full dataset size: {len(users_artists)}\")\n",
    "print(f\"Train set size: {len(train_df)} -> {len(train_df)/len(users_artists):.2f}\")\n",
    "print(f\"Train set size: {len(test_df)} -> {len(test_df)/len(users_artists):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3a3d1",
   "metadata": {},
   "source": [
    "### Artists - Tags Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90979160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artists-tags dict\n",
    "tag_N = len(tag_ids)\n",
    "artists_tags_dict = {k: np.full(tag_N,np.nan) for k in cb_artist_ids}\n",
    "# Mapping between the tag_id actual values and their indexes in a list\n",
    "tagmap = {tag_id: tag_idx for tag_idx, tag_id in enumerate(tag_ids)}\n",
    "\n",
    "grouped = (users_taggedartists.groupby(['artistID','tagID']).size().to_dict())\n",
    "for (artist_id,tag_id) , count in grouped.items():\n",
    "    artists_tags_dict[artist_id][tagmap[tag_id]] = count\n",
    "\n",
    "for artist_id, raw_tag_counts in artists_tags_dict.items():\n",
    "    artists_tags_dict[artist_id] = raw_tag_counts/np.nanmax(raw_tag_counts)\n",
    "\n",
    "# Dict -> DF -> numpy Array for better calculations\n",
    "intermediate_df = pd.DataFrame(data=artists_tags_dict)\n",
    "array = np.array(intermediate_df)\n",
    "\n",
    "N = len(cb_artist_ids)\n",
    "for idx, tag_tfs in enumerate(array):\n",
    "    idf = np.log(N/np.sum(~np.isnan(tag_tfs)))\n",
    "    array[idx] = tag_tfs * idf\n",
    "\n",
    "\n",
    "# Back to DF for interpretability\n",
    "artists_tags_df = pd.DataFrame(data=array.transpose(), index=list(cb_artist_ids))\n",
    "artists_tags_df.columns = list(tag_ids)\n",
    "\n",
    "# artists_tags_df.to_csv('./data/artists_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f7f73",
   "metadata": {},
   "source": [
    "### Users - Artists - Listening times for Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c814295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Mapping the list index of the artistIDs to the actual values\n",
    "cb_artist_reverse_map = {idx : artist_id for idx, artist_id in enumerate(cb_artist_ids)}\n",
    "\n",
    "# Dict to keep each user's training set interactions PLUS the minmax scaler fit for their specific listening times\n",
    "user_weights = {}\n",
    "for idx, row in train_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    log_weight = np.log1p(weight)   # Log scaling to keep the impact of very high weights \n",
    "\n",
    "    if(user_id in user_weights.keys()):\n",
    "        user_weights[user_id]['weights'][artist_id] = log_weight\n",
    "    else:\n",
    "        user_weights[user_id] = {\n",
    "            'weights': {\n",
    "                artist_id: log_weight\n",
    "            },\n",
    "            'scaler' : MinMaxScaler()\n",
    "        }\n",
    "\n",
    "# Fit each user's minmax scaler and transform the training log weights\n",
    "# for user_id, items in user_weights.items():\n",
    "#     weights = items['weights']\n",
    "#     scaler = items['scaler']\n",
    "    \n",
    "#     scaler.fit(np.array(list(weights.values())).reshape(-1,1))    # Scaling on the weights to help with rating prediction\n",
    "\n",
    "#     for artist_id, weight in weights.items():\n",
    "#         user_weights[user_id]['weights'][artist_id] = scaler.transform(np.array(weight).reshape(1,-1))\n",
    "\n",
    "\n",
    "# Now create a dict to keep each user's test set interactions scaled by their specific scaler\n",
    "user_weights_test = {}\n",
    "for idx, row in test_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    scaler = user_weights[user_id]['scaler']\n",
    "    log_weight = np.log1p(weight)   # Log scaling to keep the impact of very high weights \n",
    "    # scaled_weight = scaler.transform(np.array(log_weight).reshape(1,-1))\n",
    "\n",
    "\n",
    "    if(user_id in user_weights_test.keys()):\n",
    "        user_weights_test[user_id]['weights'][artist_id] = log_weight\n",
    "    else:\n",
    "        user_weights_test[user_id] = {\n",
    "            'weights': {                        # Keeping dict structure consistent between train and test\n",
    "                artist_id: log_weight\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff293496",
   "metadata": {},
   "source": [
    "## Build user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea85772",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = {}\n",
    "\n",
    "for user_id in user_weights.keys():\n",
    "    user_profile = np.zeros(len(tag_ids))\n",
    "    for artist_id, weight in user_weights[user_id]['weights'].items():\n",
    "\n",
    "        # artist has to be tagged\n",
    "        if(artist_id in cb_artist_ids):\n",
    "            artist_profile = np.nan_to_num(artists_tags_df.loc[artist_id],0)\n",
    "            user_profile += weight.item() * artist_profile\n",
    "\n",
    "    user_profiles[user_id] = user_profile.reshape(1,-1)\n",
    "\n",
    "# np.save('./data/user_profiles.npy', user_profiles, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862b84b",
   "metadata": {},
   "source": [
    "### CBF Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b425a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The recommendation method\n",
    "def recommend_cbf(user_id,k=1,new_only=True):\n",
    "    if(user_id in user_ids):\n",
    "        recommendations = { 'artist_ids': [], 'similarities': []}\n",
    "        user_profile = user_profiles[user_id]\n",
    "\n",
    "        similarities = cosine_similarity(user_profile.reshape(1,-1),artists_tags_df.fillna(0))   # Returns cos similarities with every row\n",
    "        top_sim = np.argsort(similarities[0])[::-1]                                              # Sorts the indexes in descending order\n",
    "\n",
    "        count = 0\n",
    "        i = 0\n",
    "\n",
    "        while count < k and i<len(top_sim):\n",
    "            idx = top_sim[i]\n",
    "            artist_id = cb_artist_reverse_map[idx]\n",
    "\n",
    "            # Choose whether the recommendation is something that the user has never interacted with\n",
    "            if (new_only):\n",
    "                if (artist_id not in user_weights[user_id]['weights'].keys()):\n",
    "                    recommendations['artist_ids'].append(artist_id)\n",
    "                    recommendations['similarities'].append(similarities[:,idx])\n",
    "                    count+=1\n",
    "\n",
    "            # Otherwise recommendations may contain artists the user has already interacted with\n",
    "            else:\n",
    "                recommendations['artist_ids'].append(artist_id)\n",
    "                recommendations['similarities'].append(similarities[:,idx])\n",
    "                count+=1\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        return recommendations\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a24d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77b74c",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045a2ec",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2af3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the artist-user interactions with NaNs\n",
    "cf_users_weights = {user_id: np.full(len(cf_artist_ids),np.nan) for user_id in user_ids}\n",
    "\n",
    "artistmap = {artist_id:idx for idx,artist_id in enumerate(cf_artist_ids)}\n",
    "\n",
    "# Fill in the corresponding cells with the user-artist log transformed weights (from the train dataset)\n",
    "for idx, row in train_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    cf_users_weights[user_id][artistmap[artist_id]] = np.log1p(weight)\n",
    "\n",
    "# Convert dict to DF for easy kNN calculation\n",
    "cf_df = pd.DataFrame(cf_users_weights,index=list(cf_artist_ids))\n",
    "\n",
    "# Subtract the mean for each artist -- REMOVED: not as good for implicit feedback\n",
    "# means = cf_df.mean(axis='columns', skipna=True)\n",
    "# cf_df = cf_df.sub(means.values, axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6bb73",
   "metadata": {},
   "source": [
    "### k-NN calculation for each artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c12417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each artist's k-Nearest Neighbors\n",
    "\n",
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine', algorithm='brute')\n",
    "nbrs.fit(cf_df.fillna(0))\n",
    "distances, indices = nbrs.kneighbors(cf_df.fillna(0))\n",
    "\n",
    "similarities = 1 - distances[:, 1:]\n",
    "neighbor_indices = indices[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7b1e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/similarities.npy', similarities, allow_pickle=True)\n",
    "np.save('./data/neighbors.npy', neighbor_indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce990548",
   "metadata": {},
   "source": [
    "### Neighbor and Similarity Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6c3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_artist_map = {idx : artist_id for idx, artist_id in enumerate(cf_artist_ids)}\n",
    "\n",
    "mapped_neighbor_indices = np.vectorize(cf_artist_map.get)(neighbor_indices)\n",
    "\n",
    "neighbor_df = pd.DataFrame(\n",
    "    mapped_neighbor_indices,\n",
    "    columns=[f'neighbor_{i+1}' for i in range(k)],\n",
    "    index=cf_df.index\n",
    ")\n",
    "\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarities,\n",
    "    columns=[f'similarity_{i+1}' for i in range(k)],\n",
    "    index=cf_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7748c7",
   "metadata": {},
   "source": [
    "### CF recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ff55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_cf(user_id,k=1,new_only=True):\n",
    "    if(user_id in user_ids):\n",
    "        user_dict = user_weights[user_id]['weights']\n",
    "        predictions = []\n",
    "        neighbours_used = []\n",
    "        recommendations = {'artist_ids': [], 'predictions':[], 'neighbours_used':[]}\n",
    "        for artist_id in cf_artist_ids:\n",
    "            if(new_only and artist_id in user_dict.keys()):\n",
    "                predictions.append(-100)\n",
    "                neighbours_used.append(-1)\n",
    "            else:\n",
    "                neighbors = neighbor_df.loc[artist_id]\n",
    "                similarities = similarity_df.loc[artist_id]\n",
    "\n",
    "                nbr_contributions = []\n",
    "\n",
    "                for idx, nbr_artist in enumerate(neighbors):\n",
    "                    if(nbr_artist) in user_dict.keys():\n",
    "                        nbr_contributions.append(user_dict[nbr_artist] * similarities.iloc[idx])\n",
    "\n",
    "                pred_value = 0\n",
    "                if(len(nbr_contributions) > 2):\n",
    "                    pred_value = np.sum(nbr_contributions)/len(nbr_contributions)\n",
    "\n",
    "                predictions.append(pred_value)\n",
    "                neighbours_used.append(len(nbr_contributions))\n",
    "\n",
    "        top_pred_indices = np.argsort(predictions)[::-1]\n",
    "\n",
    "        for idx in top_pred_indices[:k]:\n",
    "            recommendations['artist_ids'].append(cf_artist_map[idx])\n",
    "            recommendations['predictions'].append(predictions[idx])\n",
    "            recommendations['neighbours_used'].append(neighbours_used[idx])\n",
    "            \n",
    "        return recommendations\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258a046",
   "metadata": {},
   "source": [
    "# Final Hybrid System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid(user_id, k=1, new_only=True):\n",
    "    \n",
    "    cf_rec = recommend_cf(user_id,k,new_only)\n",
    "    \n",
    "    cbf_rec = recommend_cbf(user_id,k,new_only)\n",
    "\n",
    "    num_of_interactions = np.sum(~np.isnan(cf_df[user_id]))\n",
    "    # If no collaborative data available, use pure content-based\n",
    "\n",
    "\n",
    "    if not cf_rec or num_of_interactions<5 or cf_rec['predictions'][0] < 3.5:\n",
    "        if cbf_rec:\n",
    "            return cbf_rec\n",
    "    else:\n",
    "        return cf_rec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787a612",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53ae0",
   "metadata": {},
   "source": [
    "## CF RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126ae9b",
   "metadata": {},
   "source": [
    "### Evaluation function for CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cf():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    impossible_predictions = 0\n",
    "    count=0\n",
    "    for idx,row in (test_df.iterrows()):\n",
    "        artist_id = row['artistID']\n",
    "        user_id = row['userID']\n",
    "        weight = row['weight']\n",
    "\n",
    "        y_true.append(np.log1p(weight))\n",
    "\n",
    "        neighbors = neighbor_df.loc[artist_id]\n",
    "        similarities = similarity_df.loc[artist_id]\n",
    "\n",
    "        user_dict = user_weights[user_id]['weights']\n",
    "\n",
    "        nbr_contributions = []\n",
    "\n",
    "        for idx, nbr_artist in enumerate(neighbors):\n",
    "            if(nbr_artist) in user_dict.keys():\n",
    "                nbr_contributions.append(user_dict[nbr_artist] * similarities.iloc[idx])\n",
    "\n",
    "        pred_value = 0\n",
    "\n",
    "        # If we can't find any user interaction with any of the most similar artists, the prediction is impossible\n",
    "        if(nbr_contributions):\n",
    "            pred_value = np.sum(nbr_contributions)/len(nbr_contributions)\n",
    "            y_pred.append(pred_value)\n",
    "        else:\n",
    "            impossible_predictions+=1\n",
    "            y_true.pop()\n",
    "\n",
    "\n",
    "    return y_true,y_pred, impossible_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b0071",
   "metadata": {},
   "source": [
    "### CF Evaluation results (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users impossible to predict: 279 -> 0.15\n",
      "CF RMSE: 3.9801906112721963\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_true, y_pred_cf, impossible = evaluate_cf()\n",
    "print(f\"Users impossible to predict: {impossible} -> {impossible/(len(y_true)+impossible):.2f}\")\n",
    "print(f\"CF RMSE: {root_mean_squared_error(y_true,y_pred_cf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79009cd2",
   "metadata": {},
   "source": [
    "## Precision @ top10 Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804840e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1877/1877 [2:04:52<00:00,  3.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017421417155034633\n",
      "0.018327117741076183\n",
      "0.018966435801811402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "precisions_cbf = []\n",
    "precisions_cf = []\n",
    "precisions_hybrid = []\n",
    "\n",
    "for user_id, items in tqdm(user_weights_test.items()):\n",
    "    relevant_artists = items['weights'].keys()\n",
    "    cbf_pred = recommend_cbf(user_id,10)['artist_ids']\n",
    "    cf_pred = recommend_cf(user_id,10)['artist_ids']\n",
    "    hybrid_pred = recommend_hybrid(user_id,10)['artist_ids']\n",
    "\n",
    "    hits_cbf = len(relevant_artists & cbf_pred)\n",
    "    hits_cf = len(relevant_artists & cf_pred)\n",
    "    hits_hybrid = len(relevant_artists & hybrid_pred)\n",
    "    \n",
    "    precisions_cbf.append(hits_cbf/10)\n",
    "    precisions_cf.append(hits_cf/10)\n",
    "    precisions_hybrid.append(hits_hybrid/10)\n",
    "\n",
    "precision_at_10_cf = sum(precisions_cf)/len(precisions_cf)\n",
    "precision_at_10_cbf = sum(precisions_cbf)/len(precisions_cbf)\n",
    "precision_at_10_hybrid = sum(precisions_hybrid)/len(precisions_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX POSSIBLE P@10: 0.1\n",
      "CBF P@10 : 0.017421417155034633 -> 0.1742\n",
      "CF P@10: 0.018327117741076183 -> 0.1833\n",
      "Hybrid P@10: 0.018966435801811402 -> 0.1897\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAX POSSIBLE P@10: {1/10}\")\n",
    "print(f\"CBF P@10 : {precision_at_10_cbf} -> {precision_at_10_cbf/(1/10):.4f}\")\n",
    "print(f\"CF P@10: {precision_at_10_cf} -> {precision_at_10_cf/(1/10):.4f}\")\n",
    "print(f\"Hybrid P@10: {precision_at_10_hybrid} -> {precision_at_10_hybrid/(1/10):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8cbdd",
   "metadata": {},
   "source": [
    "### P@10 Results for each mechanism\n",
    "\n",
    ">**Note:** We're using 1 sample per user in the test set\n",
    "\n",
    "**MAX POSSIBLE P@10: 0.1**  (num_of_users * 1_possible_hit)/(num_of_users*10)\n",
    "\n",
    "**CBF P@10:** 0.017421417155034633 -> 0.1742\n",
    "\n",
    "**CF P@10:** 0.018327117741076183 -> 0.1833\n",
    "\n",
    "**Hybrid P@10:** 0.018966435801811402 -> 0.1897\n",
    "\n",
    "<br>\n",
    "\n",
    "**The hybrid model predicts the users' \"left-out\" artist ~19% of the time when recommending at least 10 artists.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
