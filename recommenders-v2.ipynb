{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "863f3c51",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "97e05efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2bcf0",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "- artists: Artist metadata for visualization (artistID, name, lastfm_url, image_url)\n",
    "\n",
    "- tags: Tag metadata (tagID, name)\n",
    "- users_artists: Contains the users-artists interactions / listening minutes\n",
    "- users_taggedartists: Every row represents a tag that a single user has applied to a single artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fafce2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "\n",
    "artists = pd.read_csv(\"data/artists.dat\", delimiter='\\t')\n",
    "tags = pd.read_csv(\"data/tags.dat\", delimiter='\\t')\n",
    "users_artists = pd.read_csv(\"data/user_artists.dat\", delimiter='\\t')\n",
    "users_taggedartists = pd.read_csv(\"data/user_taggedartists.dat\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13319070",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Some key points to avoid inconsistent data:\n",
    "\n",
    "- We'll consider an artistID as valid if:\n",
    "    1. They've had minimal interaction with any userID\n",
    "    2. We have metadata for that artistID\n",
    "\n",
    "- Drop any rows with invalid artistIDs from the user-artist-tag matrix\n",
    "    - i.e. We can't do CBF with an artist we don't have listening time for or that we can't visualize later on\n",
    "\n",
    "- Make sure we have metadata for tags (for visualization later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ab7bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll consider an artistID as valid if:\n",
    "#   1. They've had minimal interaction with any userID\n",
    "#   2. We have metadata for that artistID\n",
    "valid_artists = set(users_artists['artistID']).intersection(set(artists['id']))\n",
    "\n",
    "# Drop any rows with invalid artistIDs from the user-artist-tag matrix\n",
    "#   i.e. We can't do CBF with an artist we don't have listening time for or that we can't visualize later on\n",
    "users_taggedartists = users_taggedartists[users_taggedartists['artistID'].isin(valid_artists)]\n",
    "users_artists = users_artists[users_artists['artistID'].isin(valid_artists)]\n",
    "\n",
    "\n",
    "# Make sure we have metadata for tags (for visualization later)\n",
    "valid_tags = set(users_taggedartists['tagID'])\n",
    "users_taggedartists = users_taggedartists[users_taggedartists['tagID'].isin(valid_tags)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0244c81e",
   "metadata": {},
   "source": [
    "### Load artist and user IDs\n",
    "\n",
    "- Different artistID extraction for CBF and CF\n",
    "    - In general, only a subset of the artists have tag data, i.e. CBF can't handle all artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4e9483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting off, look into the user-artist-tag table to extract artist and tag IDs\n",
    "cb_artist_ids = set(users_taggedartists['artistID'])\n",
    "tag_ids = set(users_taggedartists['tagID'])\n",
    "\n",
    "\n",
    "# Do the same for CF. Also the final userset comes from this matrix.\n",
    "#   i.e. these are the users that we'll train and test the system on\n",
    "cf_artist_ids = set(users_artists['artistID'])\n",
    "user_ids = set(users_artists['userID'])\n",
    "\n",
    "# Then cross-check cf_artists with the artist table (need metadata to visualize)\n",
    "cf_artist_ids = cf_artist_ids.intersection(set(artists['id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19059b47",
   "metadata": {},
   "source": [
    "### Train / Test split\n",
    "\n",
    "- The idea is that the test set comprises of 1 interaction per user\n",
    "    - Typically the dataset contains 50 interactions per user\n",
    "    - But the dataset is medium sized and very sparse, so a larger split leads to \"unfair\" results\n",
    "\n",
    "- **Restriction**: Do not consider artists that have less than 5 interactions\n",
    "    - It is highly unlikely that we'll be able make meaningful predictions for that user with <4 interactions documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72d065cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 92834\n",
      "Train set size: 90957 -> 0.98\n",
      "Train set size: 1877 -> 0.02\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in users_artists.groupby('userID'):\n",
    "\n",
    "    # Minimum threshold to consider a user's data impactful\n",
    "    if len(group) < 5:\n",
    "        train_list.append(group)\n",
    "    else:\n",
    "        train, test = train_test_split(group, test_size=1, random_state=42)\n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "\n",
    "# Concatenate final datasets\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(f\"Full dataset size: {len(users_artists)}\")\n",
    "print(f\"Train set size: {len(train_df)} -> {len(train_df)/len(users_artists):.2f}\")\n",
    "print(f\"Train set size: {len(test_df)} -> {len(test_df)/len(users_artists):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3a3d1",
   "metadata": {},
   "source": [
    "### Artists - Tags Dataframe\n",
    "\n",
    "1. Creates a dictionary with artistIDs as keys and empty arrays as values (with length = num_of_tags)\n",
    "\n",
    "2. Updates the arrays according to each artist's tag counts (tf)\n",
    "3. Normalizes the tag counts with each artist's max tag count (normalized tf)\n",
    "4. Converts the dictionary to a dataframe in order to easily calculate each tag's idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90979160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an artists-tags dict\n",
    "tag_N = len(tag_ids)\n",
    "artists_tags_dict = {k: np.full(tag_N,np.nan) for k in cb_artist_ids}\n",
    "# Mapping between the tag_id actual values and their indexes in a list\n",
    "tagmap = {tag_id: tag_idx for tag_idx, tag_id in enumerate(tag_ids)}\n",
    "\n",
    "grouped = (users_taggedartists.groupby(['artistID','tagID']).size().to_dict())\n",
    "for (artist_id,tag_id) , count in grouped.items():\n",
    "    artists_tags_dict[artist_id][tagmap[tag_id]] = count\n",
    "\n",
    "for artist_id, raw_tag_counts in artists_tags_dict.items():\n",
    "    artists_tags_dict[artist_id] = raw_tag_counts/np.nanmax(raw_tag_counts)\n",
    "\n",
    "# Dict -> DF -> numpy Array for better calculations\n",
    "intermediate_df = pd.DataFrame(data=artists_tags_dict)\n",
    "array = np.array(intermediate_df)\n",
    "\n",
    "N = len(cb_artist_ids)\n",
    "for idx, tag_tfs in enumerate(array):\n",
    "    idf = np.log(N/np.sum(~np.isnan(tag_tfs)))\n",
    "    array[idx] = tag_tfs * idf\n",
    "\n",
    "\n",
    "# Back to DF for interpretability\n",
    "artists_tags_df = pd.DataFrame(data=array.transpose(), index=list(cb_artist_ids))\n",
    "artists_tags_df.columns = list(tag_ids)\n",
    "\n",
    "# artists_tags_df.to_csv('./data/artists_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f7f73",
   "metadata": {},
   "source": [
    "### Users - Artists - Listening times for Train / Test\n",
    "\n",
    "- Creates user-artist-listening time dictionaries for the train and test datasets\n",
    "\n",
    "- **Applies log transformation to the weights**\n",
    "    - Listening count distributions can be very skewed\n",
    "    - Log transformation helps to reduce the heavy outliers' impact on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c814295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Mapping the list index of the artistIDs to the actual values\n",
    "cb_artist_reverse_map = {idx : artist_id for idx, artist_id in enumerate(cb_artist_ids)}\n",
    "\n",
    "# Dict to keep each user's training set interactions PLUS the minmax scaler fit for their specific listening times\n",
    "user_weights = {}\n",
    "for idx, row in train_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    log_weight = np.log1p(weight)   # Log scaling to keep the impact of very high weights \n",
    "\n",
    "    if(user_id in user_weights.keys()):\n",
    "        user_weights[user_id]['weights'][artist_id] = log_weight\n",
    "    else:\n",
    "        user_weights[user_id] = {\n",
    "            'weights': {\n",
    "                artist_id: log_weight\n",
    "            },\n",
    "            'scaler' : MinMaxScaler()\n",
    "        }\n",
    "\n",
    "# Fit each user's minmax scaler and transform the training log weights\n",
    "# for user_id, items in user_weights.items():\n",
    "#     weights = items['weights']\n",
    "#     scaler = items['scaler']\n",
    "    \n",
    "#     scaler.fit(np.array(list(weights.values())).reshape(-1,1))    # Scaling on the weights to help with rating prediction\n",
    "\n",
    "#     for artist_id, weight in weights.items():\n",
    "#         user_weights[user_id]['weights'][artist_id] = scaler.transform(np.array(weight).reshape(1,-1))\n",
    "\n",
    "\n",
    "# Now create a dict to keep each user's test set interactions scaled by their specific scaler\n",
    "user_weights_test = {}\n",
    "for idx, row in test_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    scaler = user_weights[user_id]['scaler']\n",
    "    log_weight = np.log1p(weight)   # Log scaling to keep the impact of very high weights \n",
    "    # scaled_weight = scaler.transform(np.array(log_weight).reshape(1,-1))\n",
    "\n",
    "\n",
    "    if(user_id in user_weights_test.keys()):\n",
    "        user_weights_test[user_id]['weights'][artist_id] = log_weight\n",
    "    else:\n",
    "        user_weights_test[user_id] = {\n",
    "            'weights': {                        # Keeping dict structure consistent between train and test\n",
    "                artist_id: log_weight\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff293496",
   "metadata": {},
   "source": [
    "### Build user profiles\n",
    "\n",
    "Each user's profile is the sum of the artist profiles the user has interacted with, weighted by the interaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ea85772",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profiles = {}\n",
    "\n",
    "for user_id in user_weights.keys():\n",
    "    user_profile = np.zeros(len(tag_ids))\n",
    "    for artist_id, weight in user_weights[user_id]['weights'].items():\n",
    "\n",
    "        # artist has to be tagged\n",
    "        if(artist_id in cb_artist_ids):\n",
    "            artist_profile = np.nan_to_num(artists_tags_df.loc[artist_id],0)\n",
    "            user_profile += weight.item() * artist_profile\n",
    "\n",
    "    user_profiles[user_id] = user_profile.reshape(1,-1)\n",
    "\n",
    "# np.save('./data/user_profiles.npy', user_profiles, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862b84b",
   "metadata": {},
   "source": [
    "### CBF Recommender\n",
    "\n",
    "1. Takes as input a userID and the desired number of recommendations to be returned\n",
    "\n",
    "2. Calculates the cosine similarities between the user's profile and **ALL** the artist profiles\n",
    "3. Returns the k most similar artist profiles as recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b425a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The recommendation method\n",
    "def recommend_cbf(user_id,k=1,new_only=True):\n",
    "    if(user_id in user_ids):\n",
    "        recommendations = { 'artist_ids': [], 'similarities': []}\n",
    "        user_profile = user_profiles[user_id]\n",
    "\n",
    "        similarities = cosine_similarity(user_profile.reshape(1,-1),artists_tags_df.fillna(0))   # Returns cos similarities with every row\n",
    "        top_sim = np.argsort(similarities[0])[::-1]                                              # Sorts the indexes in descending order\n",
    "\n",
    "        count = 0\n",
    "        i = 0\n",
    "\n",
    "        while count < k and i<len(top_sim):\n",
    "            idx = top_sim[i]\n",
    "            artist_id = cb_artist_reverse_map[idx]\n",
    "\n",
    "            # Choose whether the recommendation is something that the user has never interacted with\n",
    "            if (new_only):\n",
    "                if (artist_id not in user_weights[user_id]['weights'].keys()):\n",
    "                    recommendations['artist_ids'].append(artist_id)\n",
    "                    recommendations['similarities'].append(similarities[:,idx])\n",
    "                    count+=1\n",
    "\n",
    "            # Otherwise recommendations may contain artists the user has already interacted with\n",
    "            else:\n",
    "                recommendations['artist_ids'].append(artist_id)\n",
    "                recommendations['similarities'].append(similarities[:,idx])\n",
    "                count+=1\n",
    "\n",
    "            i+=1\n",
    "\n",
    "        return recommendations\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a24d13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa77b74c",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045a2ec",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "We create the artist-user interaction matrix (log transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2af3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the artist-user interactions with NaNs\n",
    "cf_users_weights = {user_id: np.full(len(cf_artist_ids),np.nan) for user_id in user_ids}\n",
    "\n",
    "artistmap = {artist_id:idx for idx,artist_id in enumerate(cf_artist_ids)}\n",
    "\n",
    "# Fill in the corresponding cells with the user-artist log transformed weights (from the train dataset)\n",
    "for idx, row in train_df.iterrows():\n",
    "    artist_id = row['artistID']\n",
    "    user_id = row['userID']\n",
    "    weight = row['weight']\n",
    "\n",
    "    cf_users_weights[user_id][artistmap[artist_id]] = np.log1p(weight)\n",
    "\n",
    "# Convert dict to DF for easy kNN calculation\n",
    "cf_df = pd.DataFrame(cf_users_weights,index=list(cf_artist_ids))\n",
    "\n",
    "# Subtract the mean for each artist -- REMOVED: not as good for implicit feedback\n",
    "# means = cf_df.mean(axis='columns', skipna=True)\n",
    "# cf_df = cf_df.sub(means.values, axis='rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6bb73",
   "metadata": {},
   "source": [
    "### k-NN calculation for each artist\n",
    "\n",
    "- We follow the item-item Collaborative Filtering approach\n",
    "\n",
    "- We use kNN to quickly and efficiently calculate each artist's 50 most similar \"neighbors\"\n",
    "    - Cosine distance as a metric, which can be very easily converted to cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "84c12417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each artist's k-Nearest Neighbors\n",
    "\n",
    "k = 50\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, metric='cosine', algorithm='brute')\n",
    "nbrs.fit(cf_df.fillna(0))\n",
    "distances, indices = nbrs.kneighbors(cf_df.fillna(0))\n",
    "\n",
    "similarities = 1 - distances[:, 1:]\n",
    "neighbor_indices = indices[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7b1e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./data/similarities.npy', similarities, allow_pickle=True)\n",
    "# np.save('./data/neighbors.npy', neighbor_indices, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce990548",
   "metadata": {},
   "source": [
    "### Neighbor and Similarity Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb093d9",
   "metadata": {},
   "source": [
    "Converts the similarity and neighbor matrices to dataframes for easier access to its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be6c3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_artist_map = {idx : artist_id for idx, artist_id in enumerate(cf_artist_ids)}\n",
    "\n",
    "mapped_neighbor_indices = np.vectorize(cf_artist_map.get)(neighbor_indices)\n",
    "\n",
    "neighbor_df = pd.DataFrame(\n",
    "    mapped_neighbor_indices,\n",
    "    columns=[f'neighbor_{i+1}' for i in range(k)],\n",
    "    index=cf_df.index\n",
    ")\n",
    "\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarities,\n",
    "    columns=[f'similarity_{i+1}' for i in range(k)],\n",
    "    index=cf_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7748c7",
   "metadata": {},
   "source": [
    "### CF recommendation system\n",
    "\n",
    "1. Takes as input a userID and the desired number of recommendations to be returned\n",
    "\n",
    "2. Begins to predict the user's listening count for each one of the artists of the dataset\n",
    "    - Artists that the user has already interacted with are handled accordingly\n",
    "3. Retrieves the artist's rows from the neighbor and similarity matrices\n",
    "4. If possible, predicts the user-artist pair's listening count\n",
    "    - The user needs to have interacted with **at least three** of the neighboring artists in order to predict a listening count\n",
    "5. Returns the k top artists according to their predicted listening counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6ff55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_cf(user_id,k=1,new_only=True):\n",
    "    if(user_id in user_ids):\n",
    "        user_dict = user_weights[user_id]['weights']\n",
    "        predictions = []\n",
    "        neighbours_used = []\n",
    "        recommendations = {'artist_ids': [], 'predictions':[], 'neighbours_used':[]}\n",
    "        for artist_id in cf_artist_ids:\n",
    "            if(new_only and artist_id in user_dict.keys()):\n",
    "                # Make already interacted artists irrelevant for the recommendation process\n",
    "                predictions.append(-100)\n",
    "                neighbours_used.append(-1)\n",
    "            else:\n",
    "                neighbors = neighbor_df.loc[artist_id]\n",
    "                similarities = similarity_df.loc[artist_id]\n",
    "\n",
    "                nbr_contributions = []\n",
    "\n",
    "                for idx, nbr_artist in enumerate(neighbors):\n",
    "                    if(nbr_artist) in user_dict.keys():\n",
    "                        nbr_contributions.append(user_dict[nbr_artist] * similarities.iloc[idx])\n",
    "\n",
    "                pred_value = 0\n",
    "                if(len(nbr_contributions) > 2):\n",
    "                    pred_value = np.sum(nbr_contributions)/len(nbr_contributions)\n",
    "\n",
    "                predictions.append(pred_value)\n",
    "                neighbours_used.append(len(nbr_contributions))\n",
    "\n",
    "        top_pred_indices = np.argsort(predictions)[::-1]\n",
    "\n",
    "        for idx in top_pred_indices[:k]:\n",
    "            recommendations['artist_ids'].append(cf_artist_map[idx])\n",
    "            recommendations['predictions'].append(predictions[idx])\n",
    "            recommendations['neighbours_used'].append(neighbours_used[idx])\n",
    "            \n",
    "        return recommendations\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258a046",
   "metadata": {},
   "source": [
    "# Final Hybrid System\n",
    "\n",
    "1. Takes as input a userID and the desired number of recommendations to be returned\n",
    "\n",
    "2. Retrieves CF's predictions\n",
    "3. Retrieves CBF's predictions\n",
    "4. Applies some filters that determine which mechanisms whether CBF's recommendations should be the ones returned:\n",
    "    - CF failed to return recommendations\n",
    "    - The requested user does not have 5 or more interactions documented in the dataset (cold-start handling)\n",
    "    - CF's isn't \"sure\" about its recommendations (the similarity score for its top recommendation is lower than a certain threshold)\n",
    "5. Otherwise, it returns CF's recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fde5a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid(user_id, k=1, new_only=True):\n",
    "    \n",
    "    cf_rec = recommend_cf(user_id,k,new_only)\n",
    "    \n",
    "    cbf_rec = recommend_cbf(user_id,k,new_only)\n",
    "\n",
    "    num_of_interactions = np.sum(~np.isnan(cf_df[user_id]))\n",
    "    # If no collaborative data available, use pure content-based\n",
    "\n",
    "\n",
    "    if not cf_rec or num_of_interactions<5 or cf_rec['predictions'][0] < 3.5:\n",
    "        if cbf_rec:\n",
    "            return cbf_rec\n",
    "    else:\n",
    "        return cf_rec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787a612",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a53ae0",
   "metadata": {},
   "source": [
    "## CF RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126ae9b",
   "metadata": {},
   "source": [
    "### Evaluation function for CF\n",
    "\n",
    "Typical CF evaluation workflow in order to calculate the mechanism's RMSE, but with a twist:\n",
    "\n",
    "- **We filter out the test samples that CF couldn't predict (no user interaction with the artist's neighbors)**\n",
    "\n",
    "- While this typically eliminates one of the mechanism's possible weaknesses and should definitely be included in a standalone evaluation process,\n",
    "it makes sense to omit it here since such cases will be handled CBF in our Hybrid System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6055d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cf():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    impossible_predictions = 0\n",
    "    count=0\n",
    "    for idx,row in (test_df.iterrows()):\n",
    "        artist_id = row['artistID']\n",
    "        user_id = row['userID']\n",
    "        weight = row['weight']\n",
    "\n",
    "        y_true.append(np.log1p(weight))\n",
    "\n",
    "        neighbors = neighbor_df.loc[artist_id]\n",
    "        similarities = similarity_df.loc[artist_id]\n",
    "\n",
    "        user_dict = user_weights[user_id]['weights']\n",
    "\n",
    "        nbr_contributions = []\n",
    "\n",
    "        for idx, nbr_artist in enumerate(neighbors):\n",
    "            if(nbr_artist) in user_dict.keys():\n",
    "                nbr_contributions.append(user_dict[nbr_artist] * similarities.iloc[idx])\n",
    "\n",
    "        pred_value = 0\n",
    "\n",
    "        # If we can't find any user interaction with any of the most similar artists, the prediction is impossible\n",
    "        if(nbr_contributions):\n",
    "            pred_value = np.sum(nbr_contributions)/len(nbr_contributions)\n",
    "            y_pred.append(pred_value)\n",
    "        else:\n",
    "            impossible_predictions+=1\n",
    "            y_true.pop()\n",
    "\n",
    "\n",
    "    return y_true,y_pred, impossible_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b0071",
   "metadata": {},
   "source": [
    "### CF Evaluation results (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e98c8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users impossible to predict: 274 -> 0.15\n",
      "CF RMSE: 3.9834017974691065\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_true, y_pred_cf, impossible = evaluate_cf()\n",
    "print(f\"Users impossible to predict: {impossible} -> {impossible/(len(y_true)+impossible):.2f}\")\n",
    "print(f\"CF RMSE: {root_mean_squared_error(y_true,y_pred_cf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79009cd2",
   "metadata": {},
   "source": [
    "## Precision @ top10 Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "precisions_cbf = []\n",
    "precisions_cf = []\n",
    "precisions_hybrid = []\n",
    "\n",
    "for user_id, items in tqdm(user_weights_test.items()):\n",
    "    relevant_artists = items['weights'].keys()\n",
    "    cbf_pred = recommend_cbf(user_id,10)['artist_ids']\n",
    "    cf_pred = recommend_cf(user_id,10)['artist_ids']\n",
    "    hybrid_pred = recommend_hybrid(user_id,10)['artist_ids']\n",
    "\n",
    "    hits_cbf = len(relevant_artists & cbf_pred)\n",
    "    hits_cf = len(relevant_artists & cf_pred)\n",
    "    hits_hybrid = len(relevant_artists & hybrid_pred)\n",
    "    \n",
    "    precisions_cbf.append(hits_cbf/10)\n",
    "    precisions_cf.append(hits_cf/10)\n",
    "    precisions_hybrid.append(hits_hybrid/10)\n",
    "\n",
    "precision_at_10_cf = sum(precisions_cf)/len(precisions_cf)\n",
    "precision_at_10_cbf = sum(precisions_cbf)/len(precisions_cbf)\n",
    "precision_at_10_hybrid = sum(precisions_hybrid)/len(precisions_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa656de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAX POSSIBLE P@10: {1/10}\")\n",
    "print(f\"CBF P@10 : {precision_at_10_cbf} -> {precision_at_10_cbf/(1/10):.4f}\")\n",
    "print(f\"CF P@10: {precision_at_10_cf} -> {precision_at_10_cf/(1/10):.4f}\")\n",
    "print(f\"Hybrid P@10: {precision_at_10_hybrid} -> {precision_at_10_hybrid/(1/10):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8cbdd",
   "metadata": {},
   "source": [
    "### P@10 Results for each mechanism\n",
    "\n",
    ">**Note:** We're using 1 sample per user in the test set\n",
    "\n",
    "**MAX POSSIBLE P@10: 0.1**  (num_of_users * 1_possible_hit)/(num_of_users*10)\n",
    "\n",
    "| Model  | Original P@10          | Adjusted P@10 |\n",
    "| :----- | :--------------------- | :------------ |\n",
    "| CBF    | 0.017421417155034633   | 0.1742        |\n",
    "| CF     | 0.018327117741076183   | 0.1833        |\n",
    "| Hybrid | 0.018966435801811402   | 0.1897        |\n",
    "\n",
    "<br>\n",
    "\n",
    "**The hybrid model predicts the users' \"left-out\" artist ~19% of the time when recommending at least 10 artists.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ae05b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e01a8e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
